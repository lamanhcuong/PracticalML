---
title: "Practical Machine Learning Course Project - Prediction"
author: "Cuong Manh La"
date: "December 25, 2016"
output: html_document
---
#Executive Summary
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise. The report describes the model being built, the cross validation, the expected out of sample error, and rational of the choices made. Then I will also use chosen prediction model to predict 20 different test cases.

#Processing Data
##Loading library and getting Data
###Loading library which are needed in the project:

```{r, echo=TRUE, results='hide', warning=FALSE}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
```

###Getting and loading data

```{r, echo=TRUE}
if (!file.exists("pml-training.csv")) {
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "pml-training.csv")
}
if (!file.exists("pml-testing.csv")) {
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "pml-testing.csv")
}
training <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testing <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
```


##Cleaning up the data and Bootstrap
###Cleanup the data
The data provided has many variables with missing data, and variables that are not relevant to the question being analyzed, so we will remove those variables from training and testing dataset.

The first 7 variables are not neccessary for analysis, exclude them from training and testing dataset:

```{r, echo=TRUE}
training <- training[, 8:ncol(training)]
testing <- testing[, 8:ncol(testing)]
```

Next remove the variables whose missing data ratio is bigger than some certain number, I choose 30% in my work:

```{r, echo=TRUE}
maxMissingValRatio <- 0.3
training_nrow <- nrow(training)
testing_nrow <- nrow(testing)

training_selNames <- names(training[,colSums(is.na(training))/training_nrow <= maxMissingValRatio])
testing_selNames <- names(testing[,colSums(is.na(testing))/testing_nrow <= maxMissingValRatio])
selNames <- c(NULL)
for (iName in training_selNames){
    if (iName %in% testing_selNames) {
        selNames <- c(selNames, iName)
    }
}

training <- training[, c(selNames, "classe")]
testing <- testing[, c(selNames, "problem_id")]
dim(training); dim(testing)
```

###Making Training & Testing Subset
We will partition the original training dataset (which has 19,622 observations) into two subsets: 60% to be used for training and the remaining 40% to be used as the testing set.

```{r, echo=TRUE}
set.seed(1234)
inTrain = createDataPartition(y=training$classe, p = .60, list=FALSE)
trainingSub = training[inTrain,]
testingSub = training[-inTrain,]
dim(trainingSub); dim(testingSub)
```

#Building Predicton Model
##Decision Tree Model

A decision tree was the first model tested with the method rpart.
```{r, echo=TRUE}
set.seed(1271)
modFitRP <- rpart(classe~., method="class", data=trainingSub)
#print(modFitRP)
fancyRpartPlot(modFitRP)
```

Then predict testing subset with the model:
```{r, echo=TRUE}
predictionRP = predict(modFitRP, testingSub, type = "class")
cfmRP <- confusionMatrix(predictionRP, testingSub$classe)
print(cfmRP)
plot(cfmRP$table, col = cfmRP$byClass, main = paste("Decision Tree Confusion Matrix: Accuracy =", round(cfmRP$overall['Accuracy'], 4)))
```

Accuracy of the decision tree model is 0.7392, a bit good but let's build the random forest model as well and compare the two models.


##Random Forest Model
Next we use random forest model to see if the model fits the data better.

```{r, echo=TRUE}
set.seed(2471)
modFitRF <- randomForest(classe ~ ., data = trainingSub)
print(modFitRF)
predictionRF <- predict(modFitRF, testingSub, type = "class")
cfmRF <- confusionMatrix(predictionRF, testingSub$classe)
print(cfmRF)
plot(cfmRF$table, col = cfmRF$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cfmRF$overall['Accuracy'], 4)))
```

##Out of Sample Error
The RF model built from training data has OOB estimate of error rate of 0.75%, and results of cross validate of the model on testing data has a 99.2% accuracy, i.e 0.8% error rate, which is close to the estimate of error rate. 

#Conclusion
Random Forest is a superior model than decision tree for prediction of project data. The RF model had over 99% accuracy and fitted well to other subsamples of the data. I will choose random forest model to predict 20 different test cases in the project Quiz.

#Use chosen model (RF) to solve project Quiz
```{r, echo=TRUE}
quizAns = predict(modFitRF, newdata=testing)
print(quizAns)
```
